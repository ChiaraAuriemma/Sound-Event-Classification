@article{emotionRecognition2021,
	abstract = {The field of Music Emotion Recognition has become and established research sub-domain of Music Information Retrieval. Less attention has been directed towards the counterpart domain of Audio Emotion Recognition, which focuses upon detection of emotional stimuli resulting from non-musical sound. By better understanding how sounds provoke emotional responses in an audience, it may be possible to enhance the work of sound designers. The work in this paper uses the International Affective Digital Sounds set. A total of 76 features are extracted from the sounds, spanning the time and frequency domains. The features are then subjected to an initial analysis to determine what level of similarity exists between pairs of features measured using Pearson's r correlation coefficient before being used as inputs to a multiple regression model to determine their weighting and relative importance. The features are then used as the input to two machine learning approaches: regression modelling and artificial neural networks in order to determine their ability to predict the emotional dimensions of arousal and valence. It was found that a small number of strong correlations exist between the features and that a greater number of features contribute significantly to the predictive power of emotional valence, rather than arousal. Shallow neural networks perform significantly better than a range of regression models and the best performing networks were able to account for 64.4{\%} of the variance in prediction of arousal and 65.4{\%} in the case of valence. These findings are a major improvement over those encountered in the literature. Several extensions of this research are discussed, including work related to improving data sets as well as the modelling processes.},
	author = {Cunningham, Stuart and Ridley, Harrison and Weinel, Jonathan and Picking, Richard},
	date = {2021/08/01},
	date-added = {2025-06-04 12:25:30 +0200},
	date-modified = {2025-06-04 12:25:30 +0200},
	doi = {10.1007/s00779-020-01389-0},
	id = {Cunningham2021},
	isbn = {1617-4917},
	journal = {Personal and Ubiquitous Computing},
	number = {4},
	pages = {637--650},
	title = {Supervised machine learning for audio emotion recognition},
	url = {https://doi.org/10.1007/s00779-020-01389-0},
	volume = {25},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s00779-020-01389-0}}

@INPROCEEDINGS{birdsCNN2017,
  author={Narasimhan, Revathy and Fern, Xiaoli Z. and Raich, Raviv},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Simultaneous segmentation and classification of bird song using CNN}, 
  year={2017},
  volume={},
  number={},
  pages={146-150},
  keywords={Birds;Spectrogram;Image segmentation;Training;Biomedical acoustics;Convolution;Decoding;Convolutional Neural Network;encoderdecoder architecture;bioacoustic species classification},
  doi={10.1109/ICASSP.2017.7952135}}

@article{DescriptiveESC2022,
title = {Environmental Sound Classification: A descriptive review of the literature},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200115},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200115},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000539},
author = {Anam Bansal and Naresh Kumar Garg},
keywords = {Environmental Sound Classification, Feature extraction, Feature selection, Machine learning classifiers, Deep neural networks},
abstract = {Automatic environmental sound classification (ESC) is one of the upcoming areas of research as most of the traditional studies are focused on speech and music signals. Classifying environmental sounds such as glass breaking, helicopter, baby crying and many more can aid in surveillance systems as well as criminal investigations. In this paper, a vast range of literature in the field of ESC is elucidated from various facets like preprocessing, feature extraction, and classification techniques. Researchers have used various noise removal and signal enhancement techniques to preprocess the signals. This paper explicates multitude of datasets used in recent studies along with the year of publication and maximum accuracy achieved with the dataset. Deep Neural Networks surpass the traditional machine learning classifiers. The future challenges and prospective research in this field are proposed. Since no recent review on ESC has been published, this study will open up novel ways for certain business applications and security systems.}
}

@INPROCEEDINGS{ReviewSoundEvent2025,
  author={Padmaja, S. and Sharmila Banu, N.},
  booktitle={2025 5th International Conference on Trends in Material Science and Inventive Materials (ICTMIM)}, 
  title={A Systematic Literature Review on Sound Event Detection and Classification}, 
  year={2025},
  volume={},
  number={},
  pages={1580-1587},
  keywords={Wavelet transforms;Accuracy;Event detection;Smart cities;Neural networks;Feature extraction;Wavelet analysis;Robustness;Spectrogram;Systematic literature review;Sound Event Detection;MFCCs;Neural Networks;Machine Learning},
  doi={10.1109/ICTMIM65579.2025.10988199}}

@misc{kumar2016weaklysupervisedscalableaudio,
      title={Weakly Supervised Scalable Audio Content Analysis}, 
      author={Anurag Kumar and Bhiksha Raj},
      year={2016},
      eprint={1606.03664},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1606.03664}, 
}

@inproceedings{piczak2015dataset,
  title={ESC: Dataset for Environmental Sound Classification},
  author={Piczak, Karol J.},
  booktitle={Proceedings of the 23rd Annual ACM Conference on Multimedia},
  pages={1015--1018},
  year={2015},
  publisher={ACM}
}

@article{salamon2017deep,
  title={Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification},
  author={Salamon, Justin and Bello, Juan Pablo},
  journal={IEEE Signal Processing Letters},
  volume={24},
  number={3},
  pages={279--283},
  year={2017},
  publisher={IEEE},
  doi={10.1109/LSP.2017.2657381}
}

@article{traer2016statistics,
  title={Statistics of natural reverberation enable perceptual separation of sound and space},
  author={Traer, James and McDermott, Josh H},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={48},
  pages={E7856--E7865},
  year={2016},
  publisher={National Academy of Sciences},
  doi={10.1073/pnas.1612524113},
  url={https://www.pnas.org/doi/10.1073/pnas.1612524113}
}

@misc{scikit-learn_stratifiedkfold,
  author = {Scikit-learn developers},
  title = {StratifiedKFold — scikit-learn 1.5.2 documentation},
  year = {2025},
  url = {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html}
}

@misc{zhang2019attentionbasedconvolutionalrecurrent,
      title={Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification}, 
      author={Zhichao Zhang and Shugong Xu and Tianhao Qiao and Shunqing Zhang and Shan Cao},
      year={2019},
      eprint={1907.02230},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1907.02230}, 
}

@misc{audio_classification_esc50,
  author       = {Enes Furkan Örnek},
  title        = {Audio Classification using CNN on ESC-50 Dataset},
  year         = {2023},
  howpublished = {\url{https://github.com/sweat0198/audio_classification_CNN_ESC-50}},
  note         = {Accessed: 2025-06-09}
}

@misc{crnn_audio_classification,
  author       = {Kiran Sanjeevan Cabeza},
  title        = {PyTorch Audio Classification: Urban Sounds},
  year         = {2019},
  howpublished = {\url{https://github.com/ksanjeevan/crnn-audio-classification}},
  note         = {Accessed: 2025-06-10}
}

@article{fonseca2020fsd50k,
  author    = {Eduardo Fonseca and Xavier Favory and Jordi Pons and Frederic Font and Xavier Serra},
  title     = {FSD50K: An Open Dataset of Human-Labeled Sound Events},
  journal   = {arXiv preprint arXiv:2010.00475},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.00475},
  note      = {Accessed: 2025-06-09}
}

@misc{latifi2025classificationheartsoundsusing,
      title={Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN}, 
      author={Seyed Amir Latifi and Hassan Ghassemian and Maryam Imani},
      year={2025},
      eprint={2407.10689},
      archivePrefix={arXiv},
      primaryClass={eess.SP},
      url={https://arxiv.org/abs/2407.10689}, 
}

@INPROCEEDINGS{Piczak2015environmental,
  author={Piczak, Karol J.},
  booktitle={2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={Environmental sound classification with convolutional neural networks}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  keywords={Neural networks;Training;Accuracy;Convolution;Convolutional codes;Yttrium;Pattern recognition;environmental sound;convolutional neural networks;classification},
  doi={10.1109/MLSP.2015.7324337}}

@Article{electronics11152279,
  author = {Ashurov, Asadulla and Zhou, Yi and Shi, Liming and Zhao, Yu and Liu, Hongqing},
  title = {Environmental Sound Classification Based on Transfer-Learning Techniques with Multiple Optimizers},
  journal = {Electronics},
  volume = {11},
  year = {2022},
  number = {15},
  article-number = {2279},
  url = {https://www.mdpi.com/2079-9292/11/15/2279},
  issn = {2079-9292},
  abstract = {The last decade has seen increased interest in environmental sound classification (ESC) due to the increased complexity and rich information of ambient sounds. The state-of-the-art methods for ESC are based on transfer learning paradigms that often utilize learned representations from common image-classification problems. This paper aims to determine the effectiveness of employing pre-trained convolutional neural networks (CNNs) for audio categorization and the feasibility of retraining. This study investigated various hyper-parameters and optimizers, such as optimal learning rate, epochs, and Adam, Adamax, and RMSprop optimizers for several pre-trained models, such as Inception, and VGG, ResNet, etc. Firstly, the raw sound signals were transferred into an image format (log-Mel spectrogram). Then, the selected pre-trained models were applied to the obtained spectrogram data. In addition, the effect of essential retraining factors on classification accuracy and processing time was investigated during CNN training. Various optimizers (such as Adam, Adamax, and RMSprop) and hyperparameters were utilized for evaluating the proposed method on the publicly accessible sound dataset UrbanSound8K. The proposed method achieves 97.25% and 95.5% accuracy on the provided dataset using the pre-trained DenseNet201 and the ResNet50V2 CNN models, respectively.},
  doi = {10.3390/electronics11152279}
}