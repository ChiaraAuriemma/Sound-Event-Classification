% --------------------------------------------------------------------------
% Template for Project Course paper; to be used with:
%          ProjectCourse21.sty  - Project Course 2021 LaTeX style file, and
%
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage{MAECapstoneCourse,amsmath,graphicx,url,times}
\usepackage{color}
\usepackage[utf8]{inputenc}
\pagenumbering{arabic}

% Example definitions.
% --------------------
\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath $#1$}}}
\newcommand{\symmat}[1]{{\mbox{\boldmath $#1$}}}

% Title.
% --------------------
\title{Sound Event Classification}

% PUT NAMES IN ALPHABETIC ORDER (consider surname first)
\name{Chiara Auriemma, Francesca Benesso, Anna Fusari, Filippo Marri}
\address{Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano\\
Piazza Leonardo Da Vinci 32, 20122 Milano, Italy\\   
\tt{[chiara.auriemma,francesca1.benesso]@mail.polimi.it}\\
\tt{[anna.fusari,filippo.marri]@mail.polimi.it}
}

\begin{document}

\ninept
\maketitle

\begin{sloppy}

\begin{abstract}
  Sound Event Classification (SEC) has become an important task in the field of
  audio processing, with applications ranging from environmental monitoring to
  human-computer interaction. Aim of this project is to develop a sound event classification system
  based on a Convolutional Neural Network (CNN) architecture training it on the ESC-50 dataset.
  At the end, the performances of the model are compared with some state-of-the-art models (QUALE?).
  [Da finire, deve essere una sorta di riassunto del progetto, con le tecniche utilizzate e i risultati ottenuti].
\end{abstract}

\begin{keywords}
Sound Event Classification, Convolutional Neural Network, ESC-50 dataset, performance limitations
\end{keywords}

\section{Introduction}
\label{sec:intro}

\subsection{Background}
\label{sec:background}
Sound Event Classification (SEC) is a task that involves classification of
specific sound events within an audio signal. This task has gained significant attention in recent years
due to its wide range of applications, including environmental monitoring \cite{birdsCNN2017}, human-computer interaction \cite{emotionRecognition2021},
and multimedia content analysis \cite{kumar2016weaklysupervisedscalableaudio}.
The goal of SEC is to accurately classify sound events in real-time or from pre-recorded audio data.
The process of SEC typically involves several steps, including feature extraction, model training, and evaluation \cite{ReviewSoundEvent2025}.
Commonly used features include Mel-frequency cepstral coefficients (MFCCs), log-spectrograms, and log-mel spectrograms.
Model training involves using labeled audio data to train a machine learning model to classify sound events.
Various machine learning algorithms can be used for SEC, including support vector machines (SVMs), decision trees, and deep
learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) \cite{DescriptiveESC2022}.
The choice of algorithm depends on the complexity of the task and the available data.
Evaluation of the SEC system is typically done using standard metrics such as accuracy, precision, recall, and F1-score.

\subsection{State of the art}
\label{sec:state_of_the_art}


\section{Methodology}
\label{sec:methodology}
As a baseline of this project we chose a simple 2D Convolutional Neural Network (CNN) architecture based on the CONV2D model of laboratory number 4.
We chose a 2D Convolutional network since we have, as input, a log mel-spectrogram. In turn, this kind of input has been chosen in order to feed the network with
temporal-frequency correalated data, approach widely followed in the literature. However, the results obtained with this architecture were not satisfactory,
so we decided to improve the model by adding some layers and using a more complex architecture. After several attempts with Recurrent Convolutional Neural Networks (CRNN),
we opted for a multi-branch convolutional neural network (MBCNN) architecture inspired by the work of Enes Furkan Örnek \cite{audio_classification_esc50, latifi2025classificationheartsoundsusing}.

\subsection{Feature extraction}
\label{sec:feature_extraction}
As hinted before, the neural network is fed with a log mel-spectrogram, computed by a specific function that takes as input
the signal preserving its original sample rate. The parameters are the following:
\begin{itemize}
    \item \textbf{N fft}: 1024 samples
    \item \textbf{Hop size}: 512 samples
    \item \textbf{Number of mel bands}: 128 (predefined parameter)
\end{itemize}
The output is a 2D array of shape (128, 1723) representing how energy in different Mel frequency bands evolves over time.

\subsection{Model description}
\label{sec:model_description}
Our implementation utilizes a bi-dimensional Convolutional Neural Network (CNN)
with four parallel branches, each extracting different levels of information from the
input data using specifically calibrated kernel sizes. 
We highlight that our model is working in a peculiar way with the 2D layers since one dimension is, layer-by-layer, alternatively
imposed equal to 1. With this method, each branch alternates between horizontal and vertical filter orientations,
enabling efficient extraction of localized patterns across both time and frequency domains.
After each Conv2D layer, we apply batch normalization and ReLU activationin order to ensure training stability and non-linearity.

After processing through these branches, the outputs are merged via element-wise addition, followed by additional convolutional
layers with larger filter depths to further abstract the combined feature maps. 
A global average pooling layer condenses the
spatial information, and the network concludes with a dense softmax layer that outputs class probabilities over 50 categories.

The model is optimized using the Adam optimizer with AMSGrad, and trained using sparse categorical cross-entropy loss.

The theoretical justification for our filter size selection derives from signal processing
principles in audio analysis. Smaller filters (1×8) effectively capture localized
patterns and high-frequency components, while larger filters (1×64) extract broader
patterns and low-frequency information.

Not only mathematical reasons justify our approach, but also biological ones. In facts, the human auditory system employs a wave bank filter mechanism
to capture specific frequency components in audio signals, with different regions of the
basilar membrane responding to distinct frequency ranges. By incorporating parallel
branches with carefully selected filter sizes, the network similarly detects patterns within specific frequency ranges, enabling multiscale
feature representation.
\section{Evaluation}
\label{sec:evaluation}

\subsection{Dataset analysis and preprocessing}
\label{sec:format}

The dataset used for this project is the well-known ESC-50 dataset \cite{piczak2015dataset}, which contains
2000 labeled isolated eviromental sound events from 50 different classes [pensiamoci], with each class containing 40 samples.
Each sound of the dataset is a mono recording available in WAV format (Ogg Vorbis
compress at 192 kbit/s) with a sample rate of 44.1 kHz and a bit depth of 16 bits.
Clips in this dataset have been manually extracted from public field recordings gathered
by the Freesound project \cite{fonseca2020fsd50k}. The resulting dataset is available under a Creative Com-
mons non-commercial license through the Harvard Dataverse project \cite{piczak2015dataset}.

According to the analysis that will be done on the results inspired by \textit{Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification} \cite{zhang2019attentionbasedconvolutionalrecurrent}, the type of soound events in the dataset can be divided into three main categories:
\begin{itemize}
    \item \textbf{Transient sounds}: this category includes sounds that have a short duration and are characterized by a sudden onset, such as the one of a glass breaking, a thunderstorm, or a firework.
    \item \textbf{Continous sounds}: this category includes sounds that have a longer duration and are characterized by a continuous or sustained sound, such as the one a vacuum cleaner, a car engine running, or pouring water.
    \item \textbf{Intermittent sounds}: this category includes sounds that have a periodic or irregular pattern, such as the one a dog barking, a bird chirping, or a man coughing.
\end{itemize}
Some examples are reported in Fig.~\ref{fig:power_spectrograms_3_sounds}, where we can see the power spectrogram of a transient sound (glass breaking), a continuous sound (vacuum cleaner), and an intermittent sound (dog barking).

\begin{figure}[ht]
  \centering
  \centerline{\includegraphics[width=\columnwidth]{Three_types_of_sound.png}}
  \caption{Power spectrogram of a transient, a continous and an intermittent sound.}
  \label{fig:power_spectrograms_3_sounds}
\end{figure}

According to what is reported on the paper in which the ESC-50 dataset is presented \cite{piczak2015dataset}, we higlight how some sounds are more difficult to classify than others,
such as the sounds of a washing machine, an helicopter, or an engine due to their similar spectrograms. If we look to Fig.~\ref{fig:Ambiguous_sounds}, we can see how the three spectrograms
are extremely similar.

\begin{figure}[ht]
  \centering
  \centerline{\includegraphics[width=\columnwidth]{Ambiguous_sounds.png}}
  \caption{Power spectrogram of the sound produced by a washing machine, an helicopter and an engine.}
  \label{fig:Ambiguous_sounds}
\end{figure}

This misclassification happens not only for machines, but for humans too. This will be taken into account in the results section, where we will see how the model performs on different classes of sounds.

It is also important to note that, even though we consider the same class, the variability of the sounds is very high, as we can see by comparing the spectrograms of three different
samples of the \textit{dog barking} class. As we can see in Fig.~\ref{fig:Dog_barking}, the three spectrograms are very different from each other. According to the classification defined at the beginngin,
the first one can be considered an impulsive sound, the second one a continuous sound, and the third one an intermittent sound. This means that, given the nature of the sound in this class,
the onset information is not so useful.

\begin{figure}[ht]
  \centering
  \centerline{\includegraphics[width=\columnwidth]{dog_barking.png}}
  \caption{Power spectrogram of three different sounds belonging to the same \textit{dog barking} class.}
  \label{fig:Dog_barking}
\end{figure}

Furthermore, we underline how some of the ambiental sounds, like the one of the wind, have no univoque structure: by breaking down their
spectrograms in their harmonic and percussive components as it is done in Fig.~\ref{fig:Dog_barking}, it is evident that the difference
it is not so clear since the two plots are almost equal.

\begin{figure}[ht]
  \centering
  \centerline{\includegraphics[width=\columnwidth]{Harmonic_Percussive_Wind.png}}
  \caption{Harmonic and percussive decomposition of the wind blowing sound.}
  \label{fig:Harmonic_Percussive_Wind}
\end{figure}

This fast analysis of the dataset has been done to understand the limitations of the model and the difficulties that it could encounter during the training phase.

A part of the dataset, 200 elements (10\% of the total), has been separated for testing. The remaining samples have been splitted according to the stratifiedkfold module of sklearn \cite{scikit-learn_stratifiedkfold} in a training and a validation set.

Drawing inspiration by the Salamon and Bello paper \cite{salamon2017deep}, five different techniques have been implemented to process the training set:
\begin{itemize}
    \item \textbf{Time Stretching (TS)}: the audio signal is stretched or compressed in time by a random factor within a specified range. A last boolean parameter crop the processed audio to the original length, so that the model can be trained on the same length of the original audio.
    \item \textbf{Pitch Shifting (PS)}: the audio signal is shifted in pitch by a random factor within a specified range. [togliere activation probability]
    \item \textbf{Background Noise (BN)}: a Gaussian noise is added to the audio signal with a specified SNR range.
    \item \textbf{Dynamic Range Compression (DRC)}: the dynamic range of the audio signal is compressed from a certain threshold with a specified ratio, attack time, and release time. [Aggiungere che è fatto con la funzione di spotify]
    \item \textbf{Convolution with Impulse Responses (CIR)}: the audio signal is convolved with the \textit{MIT Acoustical Reverberation Scene Statistics Survey} dataset of impulse response \cite{traer2016statistics} to simulate different acoustic environments.
\end{itemize}

[Ce le mettiamo le immagini?]

For all the results presented in this paper, the training dataset has been preprocessed using the following parameters:
\begin{itemize}
    \item TS: factor between 0.8 and 1.25 with an activation probability of 1
    \item PS: factor between -5 and 5 semitones and an activation probability of 1
    \item BN: SNR between 5 and 40 dB, activation probability of 1
    \item DRC: threshold of -20 dB, ratio of 4:1, attack time of 10 ms, release time of 100 ms
    \item CIR: activation probability of 1
\end{itemize}


\subsection{Evaluation metrics}
\label{sec:metrics}
The evaluation of the sound event classification system is performed using several metrics to assess its performance.
Firstly, the test accuracy, the classification reports and the confusion matrix are computed to evaluate the overall performance of the model.

To evaluate our model, we use 5 folds cross-validation, which allows us to obtain a more robust estimate of the model's performance.
Furthermore, we ensured that there is no data leakage between training and validation sets by doing data augmentation only on the
training set after the splitting.

The training of the models have been performed on the A100 GPU by Google Colab.


\section{Results}
\label{sec:results}
\subsection{Main model results}
\label{sec:main_model_results}
[tutte le immagini per ogni fold]
The average test accuracy over the 5 folds reached by the model is 86\% with an average loss of 0.63.
The performnce for each fold is reported in Table~\ref{tab:folds_results}.

[Tabella]

\begin{table}[ht]
  \centering
  \caption{Results of the main model for each fold.}
  \label{tab:folds_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Fold & Test Accuracy & Test Loss \\
    \hline
    1 & 0.85 & 0.65 \\
    2 & 0.87 & 0.60 \\
    3 & 0.88 & 0.58 \\
    4 & 0.86 & 0.62 \\
    5 & 0.85 & 0.64 \\
    \hline
    Average & 0.86 & 0.63 \\
    \hline
  \end{tabular}
\end{table}

In [Figura] the confusion matrix of the model with the best accuracy (88\%) is reported. The
labeling of the classes is reported in the appendix.


And the reoprts in the tables~\ref{tab:classification_report_fold_3} and ~\ref{tab:classification_report_fold_4}.
\begin{table}[ht]
  \centering
  \caption{Results of the main model for each fold.}
  \label{tab:folds_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Fold & Test Accuracy & Test Loss \\
    \hline
    1 & 0.85 & 0.65 \\
    2 & 0.87 & 0.60 \\
    3 & 0.88 & 0.58 \\
    4 & 0.86 & 0.62 \\
    5 & 0.85 & 0.64 \\
    \hline
    Average & 0.86 & 0.63 \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[ht]
  \centering
  \caption{Results of the main model for each fold.}
  \label{tab:folds_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Fold & Test Accuracy & Test Loss \\
    \hline
    1 & 0.85 & 0.65 \\
    2 & 0.87 & 0.60 \\
    3 & 0.88 & 0.58 \\
    4 & 0.86 & 0.62 \\
    5 & 0.85 & 0.64 \\
    \hline
    Average & 0.86 & 0.63 \\
    \hline
  \end{tabular}
\end{table}

We highlight here how precision and recall are well balanced meaning that the model is not biased towards a specific class.
\newline
By looking at the test confusion matrix, we can see that the model is able to classify correctly most of the sounds, with some exceptions.
One of them is the \textit{wind} class that is misclassified 3 times out of 4: one time classified as
\textit{train}, one as \textit{airplane} and as \textit{sea waves}. As we can notice, all the three sounds
are continuous and noisy sounds, so it is not surprising that the model has some difficulties in classifying them.
This match with what we assumed by looking at Fig.~\ref{fig:Ambiguous_sounds}: sounds with similar spectrograms are
more difficult to classify. [Articolo ESC che dimostra come anche gli uomini le sabgliano].

Another class that is misclassified is the \textit{coughing} class, which is confused with the 
\textit{sneezing} and \textit{water drops} class. We underline how all these sounds are intermittent sounds
and that they have similar spectrograms.

For what it concerns false positive, these are the errors made by the model: the sound of the \textit{airplane} and
\textit{helicopter} has been classified as \textit{wind}, the sound of \textit{sneezing} and \textit{laughing} as \textit{caughing}
and for two times the \textit{glass breaking} sound has been misclassified as \textit{can opening}. We can justify
these results by claiming the same motivation we exposed before: similar spectrograms lead to misclassification.


\subsection{Comparison: main model with different inputs}
\label{sec:comparison_1}
During the training of the main model, we tried to use different inputs to see how they affect the performance of the model.
At the end, we notice that all the results are similar, with a slight improvement in the accuracy when using the log mel-spectrogram as input.
They are reported in Table~\ref{tab:comparison_inputs}.
\begin{table}[ht]
  \centering
  \caption{Performances for different inputs.}
  \label{tab:comparison_inputs}
  \begin{tabular}{|c|c|c|}
    \hline
    Input & Test Accuracy & Test Loss \\
    \hline
    log mel-spectrogram & 0.86 & 0.63 \\
    log STFT & 0.83 & 0.71 \\
    MFCCs & 0.78 & 0.78 \\
    \hline
  \end{tabular}
\end{table}

We notice how MFCCs are the worst input. This could seems strange thinking that MFFCs are the most processed input we can use among the ones tested.
MFCCs could be effective with classical machine learning algorithms, where the features were extracted from the raw data and then used to train the model.
However, with the improvements made by neural networks, is no longer necessary perform classical feature engineering since the networks
are able to extract better features by themselves. For what we just said, we can undesrtand why even the simple
STFT works better than MFCCs.


\section{Acknowledgments}
\label{sec:ack}


This work was supported by the Politecnico di Milano,
within the framework of the Selected Topics in Music and Acoustic Engineering Course 2025.
A special thanks goes to the course instructor, Prof. JULIO JOSÉ CARABIAS ORTI, for being
one of the brightest stars in the sky of artificial intelligence.
Last but not least, we would like to thank our wallet: without those 24 euros, we would not have been able to run anything.
Thank you for your support, we are grateful for your generosity.
Grazie a tutt coloro che ci hanno supportato e sopportato.
% -------------------------------------------------------------------------
% Either list references using the bibliography style file IEEEtran.bst
\bibliographystyle{IEEEtran}
\bibliography{refs21}
\section{Appendix - Sound classes}
\label{sec:Appendix}
\begin{itemize}
    \item 0: \textit{dog barking}
    \item 1: \textit{chirping birds}
    \item 2: \textit{vacuum cleaner}
    \item 3: \textit{rain falling}
    \item 4: \textit{sea waves}
    \item 5: \textit{crackling fire}
    \item 6: \textit{glass breaking}
    \item 7: \textit{helicopter flying}
    \item 8: \textit{chainsaw cutting}
    \item 9: \textit{engine running}
    \item 10: \textit{train passing by}
    \item 11: \textit{car honking}
    \item 12: \textit{vacuum cleaner running}
    \item 13: \textit{clock ticking}
    \item 14: \textit{water drops falling}
    \item 15: \textit{water drops}
    \item 16: \textit{}
    \item 17: \textit{}
    \item 18: \textit{}
    \item 44: \textit{sneezing}
\end{itemize}

\end{sloppy}
\end{document}
